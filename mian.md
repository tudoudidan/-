# LLM(***L**arge **L**anguage **M**odel*)  
最近刷屏的DeepSeek,这些能写诗甚至编程的AI大脑,无不令我们惊叹   
但是我们在使用它的时候可能会有一些疑问:它是怎么跑起来的?只能理解$0$和$1$的计算机又是怎么做到使用人类的语言和我们交流的?难道是魔法吗?显然不是  
那接下来就让我们揭开这一神秘的面纱
## 训练
想象一下一个婴儿(ppt显示:戴着DeepSeekLogo帽子的婴儿),他是怎么学会说话和思考的?大语言模型的成长过程需要分阶段培养
### 预训练
* 首先,工作人员会收集几TB的数据.  
* 而这些数据里难免有一些错误的/有害的信息,此时就需要将这些信息通过"数据清洗"来过滤一番
* 然后通过海量阅读,AI就能学会语法规则和词语关联

在预训练之后,我们的AI如同刚认完自的低年级学生:能理解词语、句子结构,知道"太阳东升西落"等基本常识  
但是,他并没有有太多知识和教养,此时就需要我们对其进行微调进行专项提升.  
### **fine-tuning**(微调)
如果说预训练是培养通识能力的义务教育,那么微调就像是针对特定领域的大学专业教育.  
那么它具体又是怎么实现的?假设你在数学考试中函数题丢分较多,是否需要重学整本教材?显然只需针对函数题强化训练便可  
同理,预训练之后模型已具备广泛的知识基础.而假设,它处理数学问题表现不佳,那么微调时只需要用数学题相关的数据训练模型,让模型"专项突破"便可  
## Transformer
至此,模型完成了从基础知识学习到专项能力提升的全流程.但就像人类学习需要思维工具,AI的语言理解也需要特定架构支撑——这正是接下来要解析的Transformer
### Token
首先,输入的内容会被拆分成一个个小片段,这些小片段被称为Token(PPT显示token类型的表格),Token是模型处理的最小基本单元,Token通常是完整的单词、单词的一部分、标点符号甚至是单个字符,文本必须拆分成Token才能被模型处理.  
例如,这一句话"小明喜欢吃苹果(ppt逐字显示"小明喜欢吃苹果")",(特效)就会被分为"小明","喜欢吃","苹果" 

|Tonken|例子|
|:-:|:-:|
|完整的单词|`"apple"`|
|单词的一部分|`"un"`+`"happy"`$\rightarrow$`"unhappy"`|
|标点符号|`","` `"?"`|
|单个字符|汉字|

PPT以竖线分割"小明喜欢吃苹果"  
|文本:|"小明"|"喜欢吃"|"苹果"|
|-|-|-|-|
|Token:|59495|85961|19416|

### Transformer核心机制:自注意力机制(Self-Attention)
将文本转化为Token只是第一步,真正让模型理解语义的关键,在于Transformer独特的自注意力处理流程.
举个例子,小明吃的这个"苹果",究竟是指的水果,还是某个公司?(PPT显示 [苹果 , Apple的logo] 素材,并显示"?")  
(ppt显示: $\text{Attention}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V $)
这个看似复杂的数学表达式实际上在做一件非常人性化的事：根据上下文动态调整每个词语的关联权重
因此**自注意力机制**就会通过这个复杂的矩阵乘法来分析词语关系得出结论--这里的"苹果"**大概率**指水果,而"乔布斯创建了苹果"里的"苹果"**大概率**指的是某个公司  
### 多头注意力机制
为了防止模型仅凭一次自注意力机制生成的是"一家之言",便会多次使用自注意力机制,即多头注意力机制.  
打个比方,就像是课堂上的小组讨论,每一个小组成员发表不同的见解,最终整合出更优质的解决方案,通过这种多角度分析,模型能避免片面理解
## 幻觉
当我们使用大语言模型的时候,有时感觉它在一本正经的胡说八道...  
虽然Transfromer显著提升了理解准确性,这种概率驱动的机制模拟出语言能力.这种机制既带来惊艳表现,也注定了'幻觉'的必然存在...  
所以需特别注意的是,模型可能产生看似合理实则错误的"幻觉".因其回答机制是概率生成而非逻辑推演,因此关键信息务必人工核验.
# 脑机接口(Brain Computer Interface)
