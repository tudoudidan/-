# LLM(***L**arge **L**anguage **M**odel*)  
最近刷屏的DeepSeek,这些能写诗甚至编程的AI大脑,无不令我们惊叹   
但是我们在使用它的时候可能会有一些疑问:它是怎么跑起来的?只能理解 $0$ 和 $1$ 的计算机又是怎么做到使用人类的语言和我们交流的?难道是魔法吗?显然不是  
## 训练
想象一下一个婴儿(ppt显示:戴着DeepSeekLogo帽子的婴儿),他是怎么学会说话和思考的?  
同样,大语言模型的成长过程需要分阶段培养
### 预训练
* 首先,工作人员会收集几TB的数据.  
* 而这些数据里难免有一些错误的/有害的信息,此时就需要将这些信息通过"数据清洗"来过滤一番
* 然后通过海量阅读,AI就能学会语法规则和词语关联

在预训练之后,我们的AI如同小学生:理解词语的含义,知道句子结构,知道"太阳东升西落"等基本常识  
但是,他依然并没有有太多知识,此时就需要我们对其进行微调进行专项提升.  
### **fine-tuning**(微调)
如果说预训练是培养通识能力的义务教育,那么微调就像是针对特定领域的大学专业教育.  
那么它具体又是怎么实现的?假设你在数学考试中函数题丢分较多,是否需要重学整本教材?显然只需针对函数题强化训练便可  
同理,预训练之后模型已具备广泛的知识基础.而假设,它处理数学问题表现不佳,那么微调时只需要用数学题相关的数据训练模型,让模型"专项突破"便可  
## Transformer
至此,模型完成了从基础知识学习到专项能力提升的全流程.  
但就像人类学习需要思维工具,AI的语言理解也需要特定架构支撑——这正是接下来要解析的Transformer
### Token
首先,输入的内容会被拆分成一个个小片段,这些小片段被称为Token(PPT显示token类型的表格).  
Token是模型处理的最小基本单元,Token通常是完整的单词、单词的一部分、标点符号甚至是单个字符.  
文本必须拆分成Token才能被模型处理.  
例如,这一句话"小明喜欢吃苹果(ppt逐字显示"小明喜欢吃苹果")",(特效)就会被分为"小明","喜欢吃","苹果" 
那么,大家思考一下"乔布斯创立苹果"又该如何被拆分呢?
|Tonken|例子|
|:-:|:-:|
|完整的单词|`"apple"`|
|单词的一部分|`"un"`+`"happy"`$\rightarrow$`"unhappy"`|
|标点符号|`","` `"?"`|
|单个字符|汉字|

PPT以竖线分割"小明喜欢吃苹果"  
|文本:|"小明"|"喜欢吃"|"苹果"|
|-|-|-|-|
|Token:|59495|85961|19416|

### Transformer核心机制:自注意力机制(Self-Attention)
将文本转化为Token只是第一步,真正让模型理解语义的关键,在于Transformer独特的自注意力处理流程.  
那么大家思考一下,小明吃的这个"苹果",究竟是指的水果,还是某个公司?(PPT显示 [苹果 , Apple的logo] 素材,并显示"?")  
(ppt显示: $\text{Attention}(Q, K, V) = \text{softmax}(\frac{Q K^T}{\sqrt{d_k}})V $)  
因此**自注意力机制**就会上下文来分析词语关系得出这里"苹果"各含义的概率  
那么,这里(小明"喜欢吃苹果")的"苹果"**大概率**指水果,而指苹果公司的概率较小  
而"乔布斯创立苹果"里的"苹果"**大概率**指的是某个公司,而指水果的概率较小  
### 多头注意力机制
为了防止模型仅凭一次自注意力机制得出的结论太过于片面,便会多次使用自注意力机制,即多头注意力机制.  
就像是课堂上的小组讨论,每一个小组成员依据不同的角度发表不同的见解(语义/语法/情感等),最终整合出更优质的解决方案  
通过这种多角度分析,模型能避免片面理解
## 幻觉
虽然Transfromer显著提升了理解准确性,这种概率驱动的机制模拟出语言能力.这种机制既带来惊艳表现,也注定了'幻觉'的必然存在...  
因此当我们使用大语言模型的时候,有时感觉它在一本正经的胡说八道...  
所以当我们使用大语言模型时需特别注意的是,模型生成的内容可能看似合理,实则是错误的"幻觉".毕竟其回答机制是概率生成而非逻辑推演,
