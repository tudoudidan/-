
# LLM(***L**arge **L**anguage **M**odel*)  
最近刷屏的DeepSeek,这些能写诗甚至编程的AI大脑,无不令我们惊叹   
但是我们在使用它的时候可能会有一些疑问:它是怎么跑起来的?只能理解$0$和$1$的计算机又是怎么做到使用人类的语言和我们交流的?难道是魔法吗?显然不是  
那接下来就让我们揭开这一神秘的面纱
## 训练:AI的成长三部曲
想象一下一个婴儿(ppt显示:戴着DeepSeekLogo帽子的婴儿),他是怎么学会说话和思考的?大语言模型的成长过程同样需要分阶段培养
ppt显示:婴儿,印DeepSeekLogo
### 预训练--建立语言基础
* 首先,工作人员会收集几TB的数据.  
* 而这些数据里难免有一些错误的/有害的信息,此时就需要将这些信息通过"数据清洗"来去除
* 然后通过海量阅读,AI就能学会语法规则和词语关联

在预训练之后,我们的AI如同刚认完自的低年级学生:能理解词语、句子结构,知道"太阳东升西落"等基本常识  
但是,他并没有有太多知识和教养,此时就需要我们对其进行微调进行专项提升.  
### **fine-tuning**(微调)
假设你在数学考试中函数题丢分较多,是否需要重学整本教材?显然只需针对函数题强化训练便可  
同理,预训练之后模型已具备广泛的知识基础.而假设,它处理数学问题表现不佳,那么微调时只需要用数学题相关的数据训练模型,让模型"专项突破"便可  
## 核心引擎:Transformer
现在我们知道AI如何学习知识了,但它究竟怎样理解和生成语言?  
AI理解语言的关键在于Transfromer架构,让我们通过具体案例解析:
### Token
首先,输入的内容会被拆分成一个个小片段,这些小片段被称为Token(PPT显示token类型的表格),Token是模型处理的最小基本单元,Token通常是完整的单词、单词的一部分、标点符号甚至是单个字符,文本必须拆分成Token才能被模型处理.  
例如,这一句话"小明喜欢吃苹果(ppt逐字显示"小明喜欢吃苹果")",(特效)就会被分为"小明","喜欢吃","苹果" 

|Tonken|例子|
|:-:|:-:|
|完整的单词|`"apple"`|
|单词的一部分|`"un"`+`"happy"`$\rightarrow$`"unhappy"`|
|标点符号|`","` `"?"`|
|单个字符|汉字|

PPT以竖线分割"小明喜欢吃苹果"  
|文本:|"小明"|"喜欢吃"|"苹果"|
|-|-|-|-|
|Token:|59495|85961|19416|

### Transformer核心机制:自注意力机制(Self-Attention)
但问题来了,小明吃的这个"苹果",究竟是指的水果,还是某个公司?(PPT显示 [苹果 , Apple的logo] 素材,并显示"?")  
(ppt显示: $\text{Attention}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V $)此时**自注意力机制**就会通过复杂的矩阵乘法来分析词语关系得出结论--这里的"苹果"指水果,而"乔布斯创建了苹果"里的"苹果"指的是某个公司  

### 多头注意力机制
而为了防止模型仅凭一次自注意力机制生成的是"一家之言",便会多次使用自注意力机制.  
打个比方,

### 幻觉


# 脑机接口(Brain Computer Interface)
